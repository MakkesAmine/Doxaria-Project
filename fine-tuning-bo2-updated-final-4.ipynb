{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBznHUk7u5XK"
   },
   "source": [
    "importations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoEJsa0Kv0Pb"
   },
   "source": [
    "conversion de  fichier input_output.json vers un fichier d'entrainement format jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload the file\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_output_pairs.json\", \"r\") as f:\n",
    "    pairs = json.load(f)\n",
    "\n",
    "# Convert input_output_pairs.json to training_data.jsonl\n",
    "with open(\"training_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in pairs:\n",
    "        json_line = json.dumps({\"prompt\": pair[\"input\"], \"completion\": pair[\"output\"]}, ensure_ascii=False)\n",
    "        f.write(json_line + \"\\n\")\n",
    "\n",
    "print(\"Data converted to 'training_data.jsonl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A304JtdG0o_7"
   },
   "source": [
    "fine-tuning de mod√®le GPT2 Tokenizer avec le fichier training.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "with open(\"input_output_pairs.json\", \"r\") as f:\n",
    "    pairs = json.load(f)\n",
    "\n",
    "# Convert input_output_pairs.json to training_data.jsonl\n",
    "with open(\"training_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in pairs:\n",
    "        json_line = json.dumps({\"prompt\": pair[\"input\"], \"completion\": pair[\"output\"]}, ensure_ascii=False)\n",
    "        f.write(json_line + \"\\n\")\n",
    "\n",
    "print(\"Data converted to 'training_data.jsonl'.\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load dataset\n",
    "dataset = Dataset.from_json(\"training_data.jsonl\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)  # 10% validation split\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_function(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    completions = examples[\"completion\"]\n",
    "    combined = [f\"{prompt} ### {completion}\" for prompt, completion in zip(prompts, completions)]\n",
    "    tokenized = tokenizer(\n",
    "        combined,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments (disable W&B)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model_chatbot\",  # Output directory for model checkpoints\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    report_to=\"none\",  # Disable W&B\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],  # Optional: If you want evaluation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer locally\n",
    "model.save_pretrained(\"./fine_tuned_model_chatbot\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model_chatbot\")\n",
    "\n",
    "print(\"Fine-tuned model saved locally in './fine_tuned_model_chatbot'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poYQUoHB7N0V"
   },
   "source": [
    "push de modele vers huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 6 : Authentification Hugging Face (n√©cessaire une seule fois)\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_name = \"fine_tuned_model_chatbot\"  # üîÅ choisis ton propre nom\n",
    "create_repo(repo_name, private=True)  # `private=False` si tu veux qu‚Äôil soit public\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"fine_tuned_model_chatbot\")\n",
    "tokenizer.push_to_hub(\"fine_tuned_model_chatbot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ INSTALLATIONS (si n√©cessaire sur Colab)\n",
    "!pip install transformers pydantic fuzzywuzzy python-Levenshtein unidecode\n",
    "\n",
    "# üöÄ IMPORTS\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pydantic import BaseModel\n",
    "from fuzzywuzzy import fuzz\n",
    "import torch\n",
    "import unidecode\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# üöÄ Chargement du mod√®le fine-tun√©\n",
    "# Remplace par ton chemin si besoin (par exemple: \"/content/mon_modele/\")\n",
    "model_name = \"toumix/fine_tuned_model_chatbot\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# (Optionnel) Utiliser le GPU si disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# ‚úÖ Fonction de nettoyage / normalisation\n",
    "def normalize(text):\n",
    "    text = unidecode.unidecode(text.lower())\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# ‚úÖ Fonction de matching flou\n",
    "def fuzzy_match_any(text: str, choices: list, threshold: int = 70):\n",
    "    return any(fuzz.partial_ratio(text, choice) >= threshold for choice in choices)\n",
    "\n",
    "# ‚úÖ Charger le dictionnaire m√©dical\n",
    "# Assurez-vous que le fichier JSON est pr√©sent dans Colab : /content/separate_dictionaries_updated.json\n",
    "with open(\"/content/separate_dictionaries_updated.json\", \"r\") as f:\n",
    "    medical_dict = json.load(f)\n",
    "\n",
    "# ‚úÖ Normaliser maladies et sympt√¥mes\n",
    "disease_names = [normalize(disease) for disease in medical_dict.get(\"disease_dict\", [])]\n",
    "symptom_names = [\n",
    "    normalize(sym) for symptoms in medical_dict.get(\"symptoms_dict\", {}).values()\n",
    "    for sym in symptoms\n",
    "]\n",
    "\n",
    "# ‚úÖ Mots-cl√©s indicateurs (normalis√©s)\n",
    "keywords_by_type = {\n",
    "    \"symptoms\": [\n",
    "        \"symptom\", \"symptoms\", \"sign\", \"signs\", \"disorder\", \"illness\", \"condition\", \"i feel\",\n",
    "        \"i have\", \"i‚Äôm experiencing\", \"i am experiencing\", \"i suffer from\", \"i feel pain\",\n",
    "        \"my body hurts\", \"i feel unwell\", \"i feel bad\", \"i am not feeling well\",\n",
    "        \"what am i experiencing\", \"what is wrong with me\", \"i'm feeling\", \"something is wrong with\"\n",
    "    ],\n",
    "    \"precaution\": [\n",
    "        \"prevent\", \"prevention\", \"how to avoid\", \"how can i avoid\", \"how can i prevent\",\n",
    "        \"protection\", \"protect\", \"how to stay safe\", \"stay safe\", \"risk reduction\",\n",
    "        \"precaution\", \"precautions\", \"how to reduce risk\", \"safety measures\",\n",
    "        \"avoid getting\", \"what to do to avoid\", \"how to be careful\", \"what should i do to protect\"\n",
    "    ],\n",
    "    \"diet\": [\n",
    "        \"diet\", \"nutrition\", \"eating\", \"meal\", \"meals\", \"what to eat\", \"what should i eat\",\n",
    "        \"what food\", \"what can i eat\", \"dietary\", \"foods to avoid\", \"diet plan\",\n",
    "        \"recommend food\", \"avoid food\", \"nutritional advice\", \"what is allowed to eat\",\n",
    "        \"diet recommendation\", \"can i eat\", \"healthy food\", \"food habits\"\n",
    "    ],\n",
    "    \"medication\": [\n",
    "        \"medication\", \"medicine\", \"drug\", \"treatment\", \"remedy\", \"therapy\",\n",
    "        \"what can i take\", \"what should i take\", \"what is prescribed\", \"prescription\",\n",
    "        \"dosage\", \"dose\", \"cure\", \"take for\", \"pill\", \"tablet\", \"pharmaceutical\",\n",
    "        \"what is the best treatment\", \"medications\", \"how to treat\", \"how to cure\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ‚úÖ Fonction de validation intelligente\n",
    "def is_valid_medical_question(prompt: str) -> bool:\n",
    "    prompt_clean = normalize(prompt)\n",
    "\n",
    "    # Tous les mots-cl√©s √† plat et normalis√©s\n",
    "    all_keywords = sum(keywords_by_type.values(), [])\n",
    "    all_keywords_clean = [normalize(k) for k in all_keywords]\n",
    "\n",
    "    keyword_found = (\n",
    "        any(kw in prompt_clean for kw in all_keywords_clean) or\n",
    "        fuzzy_match_any(prompt_clean, all_keywords_clean, threshold=70)\n",
    "    )\n",
    "\n",
    "    medical_terms_clean = disease_names + symptom_names\n",
    "    medical_term_found = (\n",
    "        any(term in prompt_clean for term in medical_terms_clean) or\n",
    "        fuzzy_match_any(prompt_clean, medical_terms_clean, threshold=70)\n",
    "    )\n",
    "\n",
    "    return keyword_found and medical_term_found\n",
    "\n",
    "# ‚úÖ Mod√®le de requ√™te\n",
    "class ChatRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "# ‚úÖ G√©n√©ration de la r√©ponse\n",
    "def generate_response(prompt: str) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mettre sur GPU si dispo\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ‚úÖ Fonction chat\n",
    "def chat(request: ChatRequest):\n",
    "    if not is_valid_medical_question(request.question):\n",
    "        return {\n",
    "            \"response\": \"‚ùå Sorry, I only answer medical questions that contain a valid keyword and a known disease or symptom.\"\n",
    "        }\n",
    "    return {\"response\": generate_response(request.question)}\n",
    "\n",
    "# ‚úÖ TESTER LE CHAT\n",
    "# Exemple d'appel\n",
    "user_question = \"I have a sore throat and a cough, what should I take?\"\n",
    "request = ChatRequest(question=user_question)\n",
    "response = chat(request)\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_M0Sici7e9L"
   },
   "source": [
    "pr√©diction avec le modele d√©ja pr√©entrain√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_medical_chatbot_model\")\n",
    "tokenizer.save_pretrained(\"my_medical_chatbot_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Cr√©er l'archive\n",
    "shutil.make_archive(\"my_medical_chatbot_model\", 'zip', \"my_medical_chatbot_model\")\n",
    "\n",
    "# G√©n√©rer un lien pour t√©l√©charger l'archive\n",
    "FileLink(\"my_medical_chatbot_model.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Sauvegarder le mod√®le et le tokenizer dans un seul fichier\n",
    "with open(\"medical_chatbot_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer\n",
    "    }, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
